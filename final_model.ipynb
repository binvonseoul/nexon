{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(cwd):\n",
    "    \n",
    "    ### Data Read\n",
    "    \n",
    "    if cwd == ps.path.join(os.getcwd(),  'UX선행기획팀'):\n",
    "        data_dir = cwd\n",
    "    else:\n",
    "        data_dir = os.path.join(cwd, 'UX선행기획팀')\n",
    "        \n",
    "    output = pd.read_csv(os.path.join(data_dir, 'output_sample.csv'))\n",
    "    ## Read game datasets\n",
    "    game_A = pd.read_csv(os.path.join(data_dir, 'game_A.csv'))\n",
    "    game_B = pd.read_csv(os.path.join(data_dir, 'game_B.csv'))\n",
    "    game_C = pd.read_csv(os.path.join(data_dir, 'game_C.csv'))\n",
    "\n",
    "    ## Create id list\n",
    "    user_A = set(game_A['id'])\n",
    "    user_B = set(game_B['id'])\n",
    "    user_C = set(game_C['id'])\n",
    "\n",
    "    user_AnB = set(game_A['id']) & set(game_B['id'])\n",
    "    user_AnC = set(game_A['id']) & set(game_C['id'])\n",
    "    \n",
    "    # Convert 'sdate' feature to datetime type.\n",
    "    game_A['sdate'] = pd.to_datetime(game_A['sdate'])\n",
    "    game_B['sdate'] = pd.to_datetime(game_B['sdate'])\n",
    "    game_C['sdate'] = pd.to_datetime(game_C['sdate'])\n",
    "\n",
    "    game_A = game_A.sort_values(by='sdate', ascending=True)\n",
    "    game_B = game_B.sort_values(by='sdate', ascending=True)\n",
    "    game_C = game_C.sort_values(by='sdate', ascending=True)\n",
    "\n",
    "    features = ['on_count']\n",
    "    for col in game_A.columns:\n",
    "        if (col != 'sdate') & (col != 'id'):\n",
    "            features.append(str(\"cum_\"+col))\n",
    "\n",
    "    features.append('weekday_cum_playtime')\n",
    "    features.append('weekend_cum_playtime')\n",
    "    features.append('total_cum_playtime_B')\n",
    "    features.append('total_cum_playtime_C')\n",
    "\n",
    "    df_A = pd.DataFrame(np.nan, index=user_A, columns=features)\n",
    "\n",
    "    # Compute weekdays & week-end average play time(apt) in game A\n",
    "    # Compute  cumulative play time(cpt) and average play time(apt) in game A\n",
    "    weekday = game_A.loc[game_A['sdate'].dt.weekday < 5]\n",
    "    weekend = game_A.loc[game_A['sdate'].dt.weekday >= 5]\n",
    "    wd_apt = pd.DataFrame(weekday['totalplaytime'].groupby(weekday['id']).sum())\n",
    "    we_apt = pd.DataFrame(weekend['totalplaytime'].groupby(weekend['id']).sum())\n",
    "    weekday_users = list(set(weekday['id']))\n",
    "    weekend_users = list(set(weekend['id']))\n",
    "\n",
    "    for _id in tqdm(weekday_users):\n",
    "        df_A.loc[_id, 'weekday_cum_playtime'] = wd_apt.loc[_id][0]\n",
    "\n",
    "    for _id in tqdm(weekend_users):\n",
    "        df_A.loc[_id, 'weekend_cum_playtime'] = we_apt.loc[_id][0]\n",
    "\n",
    "    for col in game_A:\n",
    "        if col == 'id':\n",
    "            pass\n",
    "        elif col == 'sdate':\n",
    "            D = pd.DataFrame(game_A[col].groupby(game_A['id']).count())\n",
    "            print('on_count')\n",
    "            for _id in tqdm(user_A):\n",
    "                df_A.loc[_id, 'on_count'] = D.loc[_id][0]\n",
    "        else:\n",
    "            feat = str('cum_'+col)\n",
    "            D = pd.DataFrame(game_A[col].groupby(game_A['id']).sum())\n",
    "            print(feat)\n",
    "            for _id in tqdm(user_A):\n",
    "                if pd.isna(D.loc[_id,:])[0] == True:\n",
    "                    df_A.loc[_id, feat] = 0.0\n",
    "                else:\n",
    "                    df_A.loc[_id, feat] = D.loc[_id][0]\n",
    "\n",
    "    game_B_cumPlaytime = pd.DataFrame(game_B['totalplaytime'].groupby(game_B['id']).sum())\n",
    "    for _id in tqdm(user_AnB):\n",
    "        df_A.loc[_id, 'total_cum_playtime_B'] = game_B_cumPlaytime.loc[_id][0]\n",
    "\n",
    "    game_C_cumPlaytime = pd.DataFrame(game_C['totalplaytime'].groupby(game_C['id']).sum())\n",
    "    for _id in tqdm(user_AnC):\n",
    "        df_A.loc[_id, 'total_cum_playtime_C'] = game_C_cumPlaytime.loc[_id][0]\n",
    "\n",
    "    allday_player = set(df_A[df_A['on_count'] >= 31.0].index)\n",
    "    for _id in tqdm(user_A):\n",
    "        if _id in allday_player:\n",
    "            df_A.loc[_id, 'is_allday_player'] = 1.0\n",
    "        else:\n",
    "            df_A.loc[_id, 'is_allday_player'] = 0.0\n",
    "\n",
    "    # Add day averaged value\n",
    "    for col in df_A.columns:\n",
    "        if col.startswith('cum_'):\n",
    "            df_A[str(\"avg_\"+col[4:])] = df_A[col]/df_A['on_count']\n",
    "\n",
    "    df_A['weekday_day_playtime'] = df_A['weekday_cum_playtime']/df_A['on_count']\n",
    "    df_A['weekend_day_playtime'] = df_A['weekend_cum_playtime']/df_A['on_count']\n",
    "    \n",
    "    if ('passionate_B' in df_A.columns) or ('passionate_C' in df_A.columns):\n",
    "    df_A = df_A.drop(['passionate_B', 'passionate_C'], axis=1)\n",
    "\n",
    "    # Cumulative Play time top 75% user id list\n",
    "    likeB_id = df_A[df_A['total_cum_playtime_B'] >= df_A.loc[user_AnB, 'total_cum_playtime_B'].quantile(.75)].index\n",
    "    likeC_id = df_A[df_A['total_cum_playtime_C'] >= df_A.loc[user_AnC, 'total_cum_playtime_C'].quantile(.75)].index\n",
    "\n",
    "    # Assign label 1 to Cumulative Play time top 25% users, else 0.\n",
    "    df_A.loc[likeB_id, 'passionate_B'] = 1\n",
    "    df_A.loc[likeC_id, 'passionate_C'] = 1\n",
    "    \n",
    "    A = df_A.replace(np.nan, 0)\n",
    "    \n",
    "    ### Feature Engineering\n",
    "    \n",
    "    #1. Who bored at game A >>> high game online count (top 75%), low average max_level, constant low average quest count(non-zero)\n",
    "    bored_id = A[(A['on_count'] > A['on_count'].quantile(.75)) & (A['avg_max_level'] < A['avg_max_level'].quantile(.50)) & (A['avg_quest_count'] < A['avg_quest_count'].quantile(.5)) & (A['avg_quest_count'] > 0.0)].index\n",
    "    A.loc[bored_id,'bored_at_A'] = 1\n",
    "\n",
    "    #2. Who is hardcore player >>> high max_level, but high dead_count\n",
    "    hardcore_id = A[(A['avg_max_level'] > A['avg_max_level'].quantile(.90)) & (A['avg_dead_count'] >= A['avg_dead_count'].quantile(.95))].index\n",
    "    A.loc[hardcore_id,'hardcore_player'] = 1\n",
    "\n",
    "    #3. Who enjoy the storyline. >>> high npc_count and high quest_count\n",
    "    story_id = A[(A['avg_npc_count'] >= A['avg_npc_count'].quantile(.90)) & (A['avg_quest_count'] >= A['avg_quest_count'].quantile(.90))].index\n",
    "    A.loc[story_id, 'story_player'] = 1\n",
    "\n",
    "    #4. Who spend gamemoney a lot >>> high gamemoneyuse_count\n",
    "    moneyspender_id = A[(A['avg_gamemoneyuse_count'] >= A['avg_gamemoneyuse_count'].quantile(.95))].index\n",
    "    A.loc[moneyspender_id, 'gamemoney_spender'] = 1\n",
    "\n",
    "    A = A.replace(np.nan, 0)\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    for y_col in ['passionate_B', 'passionate_C']:\n",
    "\n",
    "        if y_col == 'passionate_B':\n",
    "            train = A.loc[user_AnB].drop(['total_cum_playtime_B'], axis=1).copy()\n",
    "            test = A.drop(['total_cum_playtime_B'], axis=1).copy()\n",
    "            test = test.drop(user_AnB, axis=0)\n",
    "\n",
    "        if y_col == 'passionate_C':\n",
    "            train = A.loc[user_AnC].drop(['total_cum_playtime_C'], axis=1).copy()\n",
    "            test = A.drop(['total_cum_playtime_C'], axis=1).copy()\n",
    "            test = test.drop(user_AnC, axis=0)\n",
    "\n",
    "        sc = StandardScaler()\n",
    "\n",
    "        train_y = train[y_col].copy()\n",
    "        train_x = train.drop([y_col], axis=1).copy()\n",
    "        train_x = sc.fit_transform(train_x)\n",
    "\n",
    "        test_x = test.drop([y_col], axis=1).copy()\n",
    "        test_x = sc.fit_transform(test_x)\n",
    "\n",
    "        ## Oversampling for Class-imbalanced Problem\n",
    "        from imblearn.over_sampling import SMOTE \n",
    "        sm = SMOTE(random_state=42, k_neighbors=7)\n",
    "\n",
    "        train_x, train_y = sm.fit_resample(train_x, train_y)\n",
    "\n",
    "        rf = RandomForestClassifier(random_state=0)\n",
    "        rf.fit(train_x, train_y)\n",
    "        pred_y = rf.predict(test_x)\n",
    "\n",
    "        A.loc[test.index,y_col] = pred_y\n",
    "        \n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras.optimizers import Adam, SGD\n",
    "    from tensorflow.keras.layers import Dropout\n",
    "\n",
    "    from tensorflow.keras.layers import Activation\n",
    "    from tensorflow.keras.layers import Conv1D\n",
    "    from tensorflow.keras.layers import Dropout\n",
    "    from tensorflow.keras.layers import Flatten\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    \n",
    "    for y_col in ['passionate_B', 'passionate_C']:\n",
    "        \n",
    "        # extract train data: user_AnB or user_AnC data\n",
    "        if y_col == 'passionate_B':\n",
    "            train = A.loc[user_AnB].drop(['total_cum_playtime_B'], axis=1).copy()\n",
    "            test = A.drop(['total_cum_playtime_B'], axis=1).copy()\n",
    "            test = test.drop(user_AnB, axis=0)\n",
    "\n",
    "        if y_col == 'passionate_C':\n",
    "            train = A.loc[user_AnC].drop(['total_cum_playtime_C'], axis=1).copy()\n",
    "            test = A.drop(['total_cum_playtime_C'], axis=1).copy()\n",
    "            test = test.drop(user_AnC, axis=0)\n",
    "\n",
    "        sc = StandardScaler()\n",
    "\n",
    "        train_y = train[y_col].copy()\n",
    "        train_x = train.drop([y_col], axis=1).copy()\n",
    "        train_x = sc.fit_transform(train_x)\n",
    "\n",
    "        test_x = test.drop([y_col], axis=1).copy()\n",
    "        test_x = sc.fit_transform(test_x)\n",
    "\n",
    "        ## Oversampling for Class-imbalanced Problem\n",
    "        from imblearn.over_sampling import SMOTE \n",
    "        sm = SMOTE(random_state=42, k_neighbors=7)\n",
    "        train_x, train_y = sm.fit_resample(train_x, train_y)\n",
    "\n",
    "        ## Model training with oversampled dataset\n",
    "        N = train_x.shape[0]\n",
    "        M = train_x.shape[1]\n",
    "        train_x = train_x.reshape(N, M, 1).copy()\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(128, 8, input_shape=(M, 1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(2048, activation='relu'))\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        optimizer = SGD(0.01)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=100)\n",
    "        history = model.fit(x=np.array(train_x),\n",
    "                          y=np.array(train_y),\n",
    "                          validation_split = 0.3,\n",
    "                          batch_size=128,\n",
    "                          epochs=1000,\n",
    "                          callbacks=[early_stop])\n",
    "\n",
    "        losses = pd.DataFrame(history.history)\n",
    "        losses.plot()\n",
    "\n",
    "        dnn = model\n",
    "\n",
    "        ## Reload dataset (without smote) Fit and Predict Probability with original dataset\n",
    "        train_y = train[y_col].copy()\n",
    "        train_x = train.drop([y_col], axis=1).copy()\n",
    "        train_x = sc.fit_transform(train_x)\n",
    "\n",
    "        test_x = test.drop([y_col], axis=1).copy()\n",
    "        test_x = sc.fit_transform(test_x)\n",
    "\n",
    "        N = train_x.shape[0]\n",
    "        M = train_x.shape[1]\n",
    "        train_x = train_x.reshape(N, M, 1).copy()\n",
    "\n",
    "        N = test_x.shape[0]\n",
    "        M = test_x.shape[1]\n",
    "        test_x = test_x.reshape(N, M, 1).copy()\n",
    "\n",
    "        prob_train = dnn.predict(train_x)\n",
    "        prob_test = dnn.predict(test_x)\n",
    "\n",
    "        ## Scale the prediction between 0 and 1\n",
    "        msc = MinMaxScaler()\n",
    "        prob_train = msc.fit_transform(prob_train)\n",
    "        prob_test = msc.fit_transform(prob_test)\n",
    "\n",
    "        feat_name = str(\"prob_\"+y_col)\n",
    "        A.loc[test.index, feat_name] = prob_test.reshape(len(test_x),)\n",
    "        A.loc[train.index, feat_name] = prob_train.reshape(len(train_x),)\n",
    "        \n",
    "    \n",
    "\n",
    "    for y_col in ['total_cum_playtime_B', 'total_cum_playtime_C']:\n",
    "\n",
    "        sc = StandardScaler()\n",
    "\n",
    "        if y_col == 'total_cum_playtime_B':\n",
    "            train = A.loc[user_AnB].copy()\n",
    "            test = A.drop(user_AnB, axis=0)\n",
    "        elif y_col == 'total_cum_playtime_C':\n",
    "            train = A.loc[user_AnC].copy()\n",
    "            test = A.drop(user_AnC, axis=0)\n",
    "\n",
    "        train_y = train[y_col].copy()\n",
    "        train_x = train.drop([y_col], axis=1).copy()\n",
    "        train_x = sc.fit_transform(train_x)\n",
    "\n",
    "        test_x = test.drop([y_col], axis=1).copy()\n",
    "        test_x = sc.fit_transform(test_x)\n",
    "\n",
    "        if y_col == 'total_cum_playtime_B':\n",
    "\n",
    "            # Select model: Gradient Boositing Regressor\n",
    "            gdb = GradientBoostingRegressor(random_state=0, n_estimators=500)\n",
    "            gdb.fit(train_x, train_y)\n",
    "            pred_train = gdb.predict(train_x)\n",
    "            pred_test = gdb.predict(test_x)\n",
    "\n",
    "        elif y_col == 'total_cum_playtime_C':\n",
    "\n",
    "            N = train_x.shape[0]\n",
    "            M = train_x.shape[1]\n",
    "            train_x = train_x.reshape(N, M, 1).copy()\n",
    "\n",
    "            N = test_x.shape[0]\n",
    "            M = test_x.shape[1]\n",
    "            test_x = test_x.reshape(N, M, 1).copy()\n",
    "\n",
    "            # Select model: CNN\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Conv1D(64, 8, input_shape=(M, 1)))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Conv1D(64, 8, input_shape=(M, 1)))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(2048, activation='relu'))\n",
    "            model.add(Dense(1024, activation='relu'))\n",
    "            model.add(Dense(1))\n",
    "            optimizer = Adam(.0001)\n",
    "            model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "\n",
    "            early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=100)\n",
    "            history = model.fit(x=np.array(train_x),\n",
    "                              y=np.array(train_y),\n",
    "                              validation_split = 0.2,\n",
    "                              batch_size=512,\n",
    "                              epochs=1000,\n",
    "                              callbacks=[early_stop])\n",
    "\n",
    "            losses = pd.DataFrame(history.history)\n",
    "\n",
    "            cnn = model\n",
    "            pred_train = cnn.predict(train_x)\n",
    "            pred_test = cnn.predict(test_x)\n",
    "\n",
    "        # processing result value: remove minus time and remove maximum monthly available time\n",
    "        pred_train[pred_train < 0] = 0\n",
    "        pred_test[pred_test < 0] = 0\n",
    "\n",
    "        pred_train[pred_train > (31*24*60)] = (31*24*60)\n",
    "        pred_test[pred_test > (31*24*60)] = (31*24*60)\n",
    "\n",
    "        feat_name = str(\"pred_\"+y_col)\n",
    "        A.loc[train.index, feat_name] = pred_train.reshape(len(train_x),)\n",
    "        A.loc[test.index, feat_name] = pred_test.reshape(len(test_x),)\n",
    "        \n",
    "    result = A[['pred_total_cum_playtime_B', 'prob_passionate_B', 'pred_total_cum_playtime_C', 'prob_passionate_C']]\n",
    "    output['A게임 유저ID'] = result.index\n",
    "    output.iloc[:,1:] = np.array(result.iloc[:,:])\n",
    "    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "output = my_model(cwd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
